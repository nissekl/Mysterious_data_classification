{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default project: Model validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import sklearn.model_selection as model_selection\n",
    "import torch.optim as optim\n",
    "from sklearn import linear_model, preprocessing, svm\n",
    "from sklearn.metrics import  accuracy_score\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first establish where our dataset is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_folder = 'training_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are told that the dataset has one file for each class *k*=0,1,2,... labeled 'Class*k*.csv'.  When that file is loaded, it produces a matrix where the rows contain the samples, the last column contains the label, and the other columns contain the features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the class files one-by-one until there are none left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 20 classes of training data\n",
      "unique labels:  [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "number of features:  20\n",
      "number of samples:  100000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# instantiate empty arrays for features and labels\n",
    "Xtr = np.array([])\n",
    "ytr = np.array([])\n",
    "k = 0 # initialize\n",
    "\n",
    "# load data from the relevant files\n",
    "while True:\n",
    "    try:\n",
    "        # load data file\n",
    "        class_k = np.loadtxt(training_folder + 'Class{:}.csv'.format(k))\n",
    "        # extract features and labels\n",
    "        class_k_features = class_k[:,:-1] # extract features\n",
    "        class_k_labels  = class_k[:,-1].astype(np.int) # labels; convert to int\n",
    "        # append the features and labels to the arrays\n",
    "        Xtr = np.vstack([Xtr,class_k_features]) if Xtr.size else class_k_features\n",
    "        ytr = np.hstack([ytr,class_k_labels]) if ytr.size else class_k_labels\n",
    "        # increment counter\n",
    "        k += 1\n",
    "    except:\n",
    "        print('loaded %i classes of training data' %k)\n",
    "        break\n",
    "\n",
    "# examine shape\n",
    "num_classes = k\n",
    "num_features = Xtr.shape[1]\n",
    "num_samples = Xtr.shape[0]\n",
    "\n",
    "print('unique labels: ', np.unique(ytr))\n",
    "print('number of features: ', num_features)\n",
    "print('number of samples: ', num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize the Data\n",
    "First, we want to standardize the data. We do this by first finding the sample mean and sample standard deviation for each feature. This prevents the units of the different features from affecting the model. Each feature will have a mean of 0 and variance 1 after the standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the mean and standard deviation of the training data\n",
    "Xtr_mean = Xtr.mean(axis=0)\n",
    "Xtr_std = Xtr.std(axis=0)\n",
    "\n",
    "#Standardize the training data\n",
    "Xtr_scale = (Xtr - Xtr_mean) / Xtr_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and validate a trained sk-learn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.22.1 when using version 0.23.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of model:  0.79254\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "sklearn_model_file = 'mlr_sklearn.sav'\n",
    "\n",
    "# load a trained instance of sklearn.linear_model.LogisticRegression\n",
    "model = pickle.load(open(sklearn_model_file,'rb'))\n",
    "\n",
    "# predict\n",
    "ytr_hat = model.predict(Xtr)\n",
    "\n",
    "# evaluate\n",
    "acc = np.mean(ytr_hat == ytr)\n",
    "print('accuracy of model: ',acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and validate a trained PyTorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of model:  0.79254\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# load a trained PyTorch model (see 'pytorch_saving_demo.ipynb')\n",
    "mlr_torch = torch.jit.load(\"./mlr_torch.pth\")\n",
    "\n",
    "# predict\n",
    "with torch.no_grad():\n",
    "    scores = mlr_torch(torch.Tensor(Xtr)).detach().numpy()   \n",
    "ytr_hat = np.argmax(scores,axis=1)\n",
    "\n",
    "# evaluate\n",
    "acc = np.mean(ytr_hat == ytr)\n",
    "print('accuracy of model: ',acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running our Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of model:  0.86993\n"
     ]
    }
   ],
   "source": [
    "sklearn_model_file = 'Logistic_model.sav'\n",
    "\n",
    "# load a trained instance of sklearn.linear_model.LogisticRegression\n",
    "model = pickle.load(open(sklearn_model_file,'rb'))\n",
    "\n",
    "\n",
    "# predict\n",
    "ytr_hat = model.predict(Xtr_scale)\n",
    "\n",
    "\n",
    "# evaluate\n",
    "acc = np.mean(ytr_hat == ytr)\n",
    "print('accuracy of model: ',acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running our SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of model:  0.87188\n"
     ]
    }
   ],
   "source": [
    "sklearn_model_file = 'SVC_model.sav'\n",
    "\n",
    "# load a trained instance of sklearn.linear_model.LogisticRegression\n",
    "model = pickle.load(open(sklearn_model_file,'rb'))\n",
    "\n",
    "\n",
    "# predict\n",
    "ytr_hat = model.predict(Xtr_scale)\n",
    "\n",
    "\n",
    "# evaluate\n",
    "acc = np.mean(ytr_hat == ytr)\n",
    "print('accuracy of model: ',acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running our SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of model:  0.92225\n"
     ]
    }
   ],
   "source": [
    "sklearn_model_file = 'SVM_model.sav'\n",
    "\n",
    "# load a trained instance of sklearn.linear_model.LogisticRegression\n",
    "model = pickle.load(open(sklearn_model_file,'rb'))\n",
    "\n",
    "\n",
    "# predict\n",
    "ytr_hat = model.predict(Xtr_scale)\n",
    "\n",
    "\n",
    "# evaluate\n",
    "acc = np.mean(ytr_hat == ytr)\n",
    "print('accuracy of model: ',acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading our trained 2-layer NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 2-Layer NN Model:  0.91718\n"
     ]
    }
   ],
   "source": [
    "#Load our model in\n",
    "NN2layer = torch.jit.load('./saved_model2NN.pth')\n",
    "\n",
    "#Predict the training accuracy\n",
    "with torch.no_grad():\n",
    "    scores = NN2layer(torch.Tensor(Xtr_scale)).detach().numpy()\n",
    "ytr_hat = np.argmax(scores,axis=1)\n",
    "\n",
    "# evaluate\n",
    "acc = np.mean(ytr_hat == ytr)\n",
    "print('Accuracy of 2-Layer NN Model: ',acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading our train CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of CNN Model:  0.90524\n"
     ]
    }
   ],
   "source": [
    "#Load our model in\n",
    "CNN = torch.jit.load('./saved_model_CNN.pth')\n",
    "\n",
    "#Predict the training accuracy\n",
    "with torch.no_grad():\n",
    "    scores = CNN(torch.Tensor(Xtr_scale).unsqueeze(1)).detach().numpy()\n",
    "ytr_hat = np.argmax(scores,axis=1)\n",
    "\n",
    "# evaluate\n",
    "acc = np.mean(ytr_hat == ytr)\n",
    "print('Accuracy of CNN Model: ',acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
